<h1>
AI Agent For The OCTI Board Game
</h1>
<hr class="publications-hr">
14/6/2021
<hr class="publications-hr">
<h3>Introduction</h3>
<p>
Board games that are converted into digital versions is a phenomena that recently has gathered momentum. Famous games such as checkers, chess, backgammon and others are already digitized. This allows players to play their favorite games any time and anywhere, without the necessity of arranging physical meetings or owning a physical copy of the game. Moreover, players, both amateur and professionals, can now play and train against computer.
</p>
<p>
The ability of playing against the computer allows human players to play without a partner and therefore makes the game more accessible. In addition, one can choose the level of difficulty for the virtual opponent or undo some several badly chosen moves. Thus, the game becomes a controlled training ground for players to develop and advance their skills. 
</p>
<p>
The OCTI board game is a relatively new one, compared to the classical games like <i>class</i> (500 yr.) or <i>go</i> (2,500 yr.), and was yet to become digitized, unlike the latter mentioned games [7, 8]. 
The game involves two players, playing against each other. Each player has four pieces (standing on a <i>base</i> locations at the beginning of the game) with octagonal shapes (thus the name of the game).
The game is played on a six by seven grid. A player wins by either landing a piece on any vacant tile of the opponent's <i>base</i> , or by capturing all opponent's pieces. The game is played in turns. In each turn, a player can either add an arrow to one his pieces 8 edges, which in turn allows the chosen piece to move ,in later turns, in the direction where the arrow points. A piece can jump over a rival piece if the position behind the piece in the direction of movement is empty. By doing so, the jumped over piece is captured or <i>eaten</i> , i.e, is removed from play. A piece can jump over as many pieces as available in a single turn (similarly to checkers, only here it is allowed not to capture if one desires). A piece can also jump over pieces of its own team only in that case the jumped over pieces are not affected.
</p>
<p>
In this paper, we develop and evaluate the performance of three artificial intelligence (AI) agents, in three levels of complexity, that play OCTI. During the work, we developed and deploy an OCTI <a href="https://teddy4445.github.io/octi_board_game">online game</a>.
This post organized as follows. First, we present the game, its rules and the web application interface used to develop a digital version of the game. Second, the three AI agents developed for the game are presented. Third, the results of evaluating the AI agents against human player and against each other are presented. Finally, we discuss the outcomes and methodologies used in this project and possible future improvements.

<h3>OCTI</h3>
<p>
OCTI is a strategy box game for two players. The OCTI game has two main styles that differ one another in the number of pieces (toys) each player has, the board's size, base locations, and action rules. In this manuscript, we develop the less popular version of four pieces, with bases located in a column and the board is 7X6 as this version does not have a digitized version online. 
</p>
<h4>Game Rules</h4>
<p>
The game starts when both players have their four pieces standing on marked home <i>base</i> locations. The game is turn-based when the green player is the first to take a turn. In each turn, the player plays by clicking on the relevant piece. Then, he can either add a direction arrow to it, or move to a desired location, as shown in the figure below. In case a player "jumps over" an adversary piece and is able to jump over additional pieces, he can do so in the same turn. The game is won by one of two ways: when a player lands a piece on one of the opponent's base tiles, or when a player manages to capture all opponent's pieces.
</p>
<br><div style="text-align: center;"><img src="/blog/images/blog_20_turns.png" width="100%" style="text-align: center; max-width: 800px;"></div><br><br>
<h4>Project structure</h4>
<p>
The game is developed as a single-page website, in pure JavaScript (JS) programming language with the <a href="https://p5js.org/">P5</a> library. The game's board is rendered once on page load and then the pieces and mouse cursor are rendered in each frame. When a player wins the game, a winning message appear and after some delay the game resets. 
</p>
<p>
The project structure is as follows:
</p>
<ol style="margin-left: 20px; line-height: 1.5 !important; margin-bottom: 1.7rem; font-size: 1.1rem !important; font-weight: 400; color: #212529;">
	<li>
		<b>sketch.js</b>: is the main file in the project that responsible for the player-game interactions and implements the P5.js logic to render the game visualization. On page load, it initializes the game and in each frame renders the board. In addition, it contains all the game's logic and events of the user's actions.
	</li>
	<li>
		<b>toy.js</b>: contains a player's toy class that has its current location on the board, color, id, and list of possible directions it can move to. 
	</li>
	<li>
		<b>game.js</b>: contains the two players as list of eight pieces (four for each player). This class manages the interaction with the AI players by providing an interface to the game's state and interpretation to the AI player's action.
	</li>
	<li>
		<b>move.js</b>: contains a data structure class that holds the data and meta data regarding an action.
	</li>
	<li>
		<b>player_move.js</b>: contains a data structure class that holds the data and meta data regarding an AI player's action.
	</li>
	<li>
		<b>global_math.js</b>: contains common mathematical operations.  
	</li>
	<li>
		<b>ai_player.js</b>: has all the AI player's classes. All these classes implements two methods: <i>do_move(game)</i> that receives an instance of the Game object and returns a <i>player_move</i> instance.
		Similarly, a <i>do_continue_jump_move(toyId, possibleMoves)</i> method which gets the current toy's id that makes the move (as an integer) and list of possible moves for this toy (as a list of <i>player_move</i> instances) and return a <i>player_move</i> instance from this list.
	</li>
	<li>
		<b>ai_util.js</b>: is a helper class, providing an easy to use interface to download and load from server files (specifically, the AI's models).
	</li>
	<li>
		<b>game_history_ai.js</b>:  is a class that operates as a logger to the game which later used to train a RL agent which is based on sampling multiple games and their results.
	</li>
	<li>
		<b>extended_game_history_ai.js</b>: inherits from <i>game_history_ai.js</i> class, and is used for logging the data of numerous games at once.
	</li>
	<li>
		<b>q_learning_policy.js</b>: is the "brain" behind the RL-based agent of the game, where the class manage two main operations. First, the online learning by updating the model's weights due to the games it participated at. Second, providing the optimal step for each game's state (with some chance to explore new states). 
	</li>
	<li>
		<b>policy_offline_learner.py</b>:  is responsible to get a list of <i>game_history</i> objects (as Jsons) and prepare the data structure later used by the <i>q_learning_policy</i> class. This file is exceptionally written in Python and not in JS for reasons of convenience. It should be noted that this file is for the development stage only and is not used during games. 
	</li>
	<li>
		<b>aivsai.js</b>: allows to run the game for two AI players playing against each other. This is essential for generating large quantities of gaming data. The data in turn is crucial for the training and development of the RL player.
	</li>
</ol>
<h4>Web application interface</h4>
<p>
The game is developed as a stand-alone pure front-end web application. The project is based on the concept of a  single page, where a single HTML page is loaded whenever a user enters the website. The content is updated via the JS source code based on the user's actions.
</p>
<p>
At first, the user is able to pick one of five options: 1) play on the same computer with another user. 2) play against an easy (greedy) AI agent. 3) play against a medium (MinMax with 3 steps look ahead) AI agent. 4) play against a hard (RL) AI agent. 5) play against an AI agent that is based on a code the user enters.
</p>
<p>
If the user picks one of the first four options, the game's main window opens with the game board and description of the moves (actions) done until this point. 
Otherwise, the user picks the fifth option and the user's AI window is open with places where the user can enter his own code for the AI agent's action picking logic while provided with the game's object and a list of all possible moves (actions) the agent is able to do.
</p>
<h3>AI Player Methodology</h3>
<p>
In order to develop several levels of AI players that a player can pick from, we decided to develop each AI agent using different algorithmic approach rather then to add constrains (e.g, computation time, number of steps forward the agent takes into consideration, etc.) to one type of AI agents [1].
We develop two deterministic algorithms based agents (greedy, MinMax) and a classical RL based agent [2].
We picked the first two level of AI agent complexity to be deterministic thanks to their productiveness which shown to be an important feature in AI agents in games, allowing a human (and AI) player to become better by trial and error [6].
Later we used these players to obtain the needed data to train the RL agent which designed to provides richer experience to the user which better mimics a human's style thanks to the exploration-optimization feature.  
</p>
<p>
This section organized as follows. We describe the development of each AI agent - greedy, MinMax, and RL. Than, we describe the evaluation method of an AI agent.
</p>
<h4>Greedy player</h4>
<p>
The greedy algorithm based player is based on finding the best state for a given game's state. The greedy algorithm gets the game's state and a list of all possible actions. The greedy algorithm evaluate each action using a heuristic metric. 
</p>
<p>
The greedy algorithm divides the possible actions into two groups of actions: <i>add direction</i> and <i>move</i>.
An <i>add direction</i> action is an action where we add a direction to a chosen piece. The <i>move</i> action includes a one step move of a toy in the direction of one of its available directions, jumping over a friendly (with the same color) toy, jumping over an enemy (with an other color) toy and by that removing it from the game board. Several jumps considered in a turn considered as several actions the agent needs to perform.
</p>
<p>
The greedy algorithm works as follows. In case of an <i>add direction</i> action, the action's score is calculated by reducing the minimal distance of the piece from the opponent's base in the next turn,
 would this piece will move in the new direction, while ignoring other players (that may prevent such move from happening). 
 In the case of <i>move</i> action, the action's score is depended on three main factors. First, reduction of the piece's minimal distance from the opponent's base. 
 Second, if the piece may jump over an enemy piece, there is a constant value added to this action's score. 
 Third, if the toy may be jumped over next step by the other player, a penalty, in the form of a constant value, is reduced from the action's score. 
 A <i>move</i> action that results in a piece either jumping over the last enemy piece or landing on opponent's base locations, this action gets the highest possible score possible.
</p>
<h4>MinMax player</h4>
<p>
The MinMax algorithm based player is an extension of the greedy algorithm as it uses the greedy algorithm to determinate the score of a single move. This algorithm extends the greedy algorithm by adding linear penalty to the number of friendly pieces that have been removed from the board and gives bonus for capturing enemy pieces.  
Another aspect that is introduced is a bonus for each added <i>direction arrow</i>, as one will preserve pieces with high directional mobility due to their capabilities and the amount of turns 'invested' into them.
</p>
<p>
Theoretically, since each action is deterministic the MinMax algorithm is able to determinate all possible games possible to be played and therefore determinate the winning strategy for any action the opposite player would take. Nevertheless, due to the enormous number of possible games it is practically unrealistic to compute all these possible games. In addition, as the algorithm computes the next step in real time, it must be fast enough such that it does not makes the human player to wait to much and harm its experience. Therefore, we compute the average time it takes to the MinMax algorithm to make a decision, given
<i>n = 1000</i> random states of the game on average as a function of number of steps ahead the algorithm covers (i.e., the depth of calculation). The results are summered in the table below.
</p>

<div style="width: 90%; text-align: center;" class="no-show-mobile">
<table class="table table-border" style="width: 100%">
  <thead>
    <tr>
      <th scope="col"> Look ahead steps [1]</th>
      <th scope="col">1</th>
      <th scope="col">2</th>
      <th scope="col">3</th>
      <th scope="col">4</th>
      <th scope="col">5</th>
      <th scope="col">6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">Average time in seconds (n=100) [t]</th>
      <td class="bg-success">0.03</td>
      <td class="bg-success">0.48</td>
      <td class="bg-success">3.21</td>
      <td class="bg-warning">87.55</td>
      <td class="bg-warning">1298.07</td>
      <td class="bg-danger">too long</td>
    </tr>
  </tbody>
</table>
</div>
<h4>RL player</h4>
<p>
The RL based player is based on sample based RL, where the value of each state is estimated from experience [10] . The states are treated as nodes in a graph, and the actions are the edges of connecting the states respectively. The value-based approach doesn't store any explicit policy, only a value function. The policy is derived directly from the value function, by picking the action the will lead to best value.
</p>
<p>
The main reason we choose to implement a sample based approach and not a more classical model-free algorithm like Q-learning is the issue of space complexity.
The requirement of sweeping the state space S for policy optimization, which in the Q space is even larger with <i>|Q| = |S x A|</i> leads to an exponential algorithmic complexity [4].
 To illustrate this the following equation gives an upper bound for Q-space magnitude <i>|Q|</i>.
</p>
<div style="text-align: center;"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAU4AAAA7CAYAAAAUy2OvAAAWYUlEQVR4Xu3dBZAtxdUH8CZAIEiw4O7uHvxhhbu7OwQIJLh7gru7S4JL4e7uTtDgBNevfv3VUJvNvXfkzn1732531daT7ek5c7rn3+f8zzk9Q/3yyy+/hNSSBpIGkgaSBgprYKgEnIV1lTomDSQNJA1EDSTgTAshaSBpIGmgpAYScJZUWOqeNJA0kDSQgDOtgaSBpIGkgZIaSMBZUmGpe9JA0kDSQALOtAaSBpIGkgZKaiABZ0mFpe5JA0kDSQMJONMaSBpIGkgaKKmBBJwlFZa6Jw0kDSQNJOBMa6CwBr799tsw3HDDhaGGGqrlNQ888EB4/vnnw3rrrReGGWaY2Ne1/p79u9kAX375Zfjtb38bf4q0b775JjzxxBNhxBFHDDPMMEP4zW9+E7777rtw5plnhqWXXjpMPPHERYZJfZIGSmlgwAJn0Re5lDb7cecvvvginHDCCWGDDTYI4403XtMnfeedd8KJJ54Y/vKXv4Tf//73sd8HH3wQtttuu/h/s88+e8Nrv/rqq3DGGWeEJZdcMrzwwgvhvffeC1tssUUuSBvs3HPPDZ9++mnYdtttw9BDDx3H//e//x3+/ve/h9133z2MMsoo/Xhm0qP1hQYGLHAedNBB4dZbbw3DDjtsAAqDBg0Ke+65Z/jd737XF/PQb+551llnhdFHHz2ssMIK8Zl+/PHHcNRRR4Vrrrkm/tkMOF988cVw6KGHhr/97W9xPk466aSw9957h5FGGqmlbn766afYb6GFFgpLLLHEf/U95ZRTwrjjjhuWX375fqPf9CDdoYEBCZyszbPPPjusscYa0SU87rjjwjrrrBMmnHDC7piVwSQFKw1AcXWnnnrqMOecc0ZrkYs7xhhjhNdffz3sscce4cEHHwx33XVX+Oijj+K/zz///MAdH2200cIyyywTLr300nDYYYdFkGPhrb/++mGWWWaJT/GPf/wjjDzyyPHPDTfcsClwAtiTTz45/sw999zxPpNNNlmuJsi08847x7kk70MPPRQOPPDAOJf33ntvuP7668P+++//qyWaO2DqkDRQQAMDEjgzvTgYCghMPvnk4Y9//GMBdfWvLv/5z3/Cww8/HPnA/fbbL3CXjzzyyHDEEUdEML3jjjvi/7/22mvhlltuidbjZpttFp5++ulw6qmnhk033TRyiEDu4IMPjjwj4Nxll13i/7Mige6KK64Y+7QCTu78McccExZffPG4qdnQWKjkAIC9G/d7rbXWCm+++WZYddVVw5Zbbhld+59//jkCOB7WtUB93333LcyZ9q8ZTk/TKQ0MaOD00gEJL33Gx3VK0d06LsvxkksuiW7yTTfdFJ599tmw0047RZ1MM800Yd555w1jjjlm5CeXW265aJV+8skn0aVGdwCnG264IWyyySZh/PHHjyC18cYbx0ANrpJ1z50+9thjo/u+2GKLNdQ1GT777LMIfp9//nkE4D/96U9hyimnbKm6Cy64IHz44YcRLP3J8iSHIBbAveKKK+KzFQk22Ujvu+++CLqDayN95JFH4vMuuuiibS+RvpC/baE7NMCNN94YaZqZZ565I3cY0MB59dVXR6uq6IvVkRno40EFZFhpwI51Of/884f55psvWojcda74H/7wh3DAAQeEKaaYIqy55prhlVdeCbfddlvsf84554RnnnkmAg3LkssObDMgAJrAyHirrLJKvM+jjz4a+7H23UNj1Qo+bbXVVpESuP/++8Pmm28erdhmrSe/KSiEDkARrLvuuhE4gbEoPVDPa0DnqquuihYyzrTVffPGKvN7WQE2IRsE3VZtfSV/VXk7fZ3NfZ999ok6tZ7rbgMaOFkrXuKBDJx1LygWFAuUxZhFuHvfI6NIWJ+sgqwBcNaXgF1eUChPbilJrGYgmme1GuvOO++MUX0eyNhjj503fK2/xzXvsMMO0dqu+pL3pfy1KqPGwd54443If3u/i6yBMrce0MD5/fffBz/tvqRlFN7f+7ICRdZnnXXWpoGgt956K7r4rNlm4NqOngDz5ZdfHnNGWcF5eaf4VS8Ya7cMcGXgjGtFa7jvk08+Gb0Y6VB4XsBdxHp96qmnwl577RUDlRNNNFGpx68qf5GbAHXexm677RYpGzw43hit8q9//Sump80000y5Oi5yL33w4vhzG2/mjRS9tlG/m2++OXLmgo510nEDGjjbmZB0bXMNCCK9+uqrMeiWl/DeCT0CNNYGK0NCfKsG6AXEuPSAq4y8//znPyNdceGFF0bgxA+zuCX+s565isY9/PDDI3WQJ4d+NpI///nPuXJnY7Ujf57ubQS46WuvvTY+ozQzWRe4auAO7FEv/m+uuebKG67Q720CfrJihkIXteiECtlxxx3DIossEvnvuloCzro0mcYZIjXAwtl+++2ji85yKtok+uN3pT9xBQEn6sc4uFXpXf4tlxToTDDBBLlDA95tttkmpohNO+20uf0zC62K/EUGB4xXXnlleO655yL/bBOStscqt8l8/PHH8d+ChuTu1iZfG4/M8qyLhknA2a2zneTquAZYVFKe3n333ZghkGcVZgKxqPGhAmlyRFmdgJMbK1NjqqmmipYr0BSwOu+882KALa+hjf7617+GccYZp5DVWVX+PDn8XhGC4J2cWpbw8ccfH59Brixqa6yxxop6A5zS0vzZTqNTRRIsdgGdGWecsZ3h/utavDkZpa3VZXX2a+CUkrD66qsH+Yq4JuZ/s2bRPvbYY3HhP/7447GbwIXJbFbtUnRmjc1lwLH05Nu4c9J1RhhhhKJDRdevZy23RcEysJN6YfuydYu+i+pA9F5uqdzUrNKpyLUyCgDj9NNPH7beeutfgbPntThOWQHWHw40j2fNrpUvyzpSRppnHVWVP+8ZATKrWSoPK1OWAeDEcfZsLOrLLrssbhB5subdEy9s4/K+uW9dAJfdl4xyluUKF+Gc8+Tt18BpF+NGcSuU3Z122mlxp8xrdlupKWqdpdC4Po8razSmBYgHk2pjBzUmV87E+d1FF10UwVr0Ny/PsFktt/uyeCy2lVZaKe/ROvr7vtZ32YeT54lPBFJFo67vv/9+zHdlFdnEGgFnVmYqn1S/MmvHxr322mtHSzYvt7OK/EV05IAWObg2k5dffrkhcOqD/1Sm7Dl7NnphrdrUmzXPNsccc/z6a9THDz/8EIN03gdUR51N9oysBfSKDa/d1q+Bk3IQzRQGwKR8ANLhhx++kN5MJtAU4ZPDWLbhzyyCDCxFky0ILxIrkQsEyIukQzWr5QbAKnW8wGU4urLPUrR/X+q7qIxZP266eWBNFbVCgCzwNG88GW74yiuvHMtU6T+L6PMillpqqfD222/H6HDR8b/++uu4TpWbimS3alXkz9MR8GKVsf6UynpWPKeAVxYUMseqzbi/3Hb/nmSSSfKGzv29+0irKhJMyx2sVwfFETYkMtdhzdYOnHZMQOCn96k0dmITQdEWxqijjvqrC9PqurJK6t0fb4LfkBQr1aFMojGLUIK3+mvNiyFJGuEvD1G0zgJr1Lg7IqsqYOy+s802WywpNIlSOoAx66UIcDar5QaoXLvMOvBvC6/O1Iuy+q9T32XvXbR/Fm0VtJGcX9SV7jk+XTuRSQpRlo4kDUr02bqwVgR6rLciHGc29iGHHBJeeumlloBeh/xFdMWqVQ1mvXPVvbusNvSGswpsPKxSnlk7TSbErrvuGpZddtn4ns4zzzy1HgmIEmPNkpk1n3kBcIcn6P10gpe5K9JqB06LiaLt4lkeFi4PL2QS5H0x7QnLfM6qNBpdV+QBivTJ3GL1zF4UvExVc52iEeEqX7KcP5PuMAm/Q57LHXRaD1dcyZ9AggWnbPHoo4+Ou6pr8WBZSaAXQf9G7o2+XMlGtdwqbPC3xmX5ANii1k0R3VXpU6e+q9y/yDWZBYLjrBLYwC/i44AuCxE4si6NZR1kjYVj3srw2DZcnJz10Iw7bFf+IjryHDYCGzHPyZF/1r13u2eznhdccMEiQzbtk+XETjfddHETW2211SptZq2EaLYhSeniWTKA8qz8bPyOA2dmKXFB1EBneXLZDsO9URLXSeD0sDhCO5qcM1yghdmb7C4y81n6Cn4n42GyPD4WJfeeW8MScW6lHEHcKks7q2IAko6vs1vbUACqnMdWXFijWm4vLMuVtSPxOqstl3jdicTyIvrJ+tSl7zL3LNPXPGYbYB6XWGbcOvoyMFh0rTb4bpa/qg5gBVzo1MYvmMWIabQhSfLXAGiR1nHgxC2qJOEi9w7M4IvuvvvuuCOrQuhtqRZ5gDJ9MlC7/fbb4y4KyMsCTG/gzFwmcuCcNAm3aArcqCPOVMiwRoCJTcLmwWLhxnlJpMLkRe4b1XLjbFAB+E0RQxaCIBTLs+xzldFj0b516Lvovcr2c7gJesP65BZ2U7OhsmDxiM1k62b5u0mXPWWRscCivPjii/8nZtFVwMkEtnOKaDc6aAHQOADi9NNPjxZZp4GTErkV6AI8ioWJ+yzTGlmcAAIdgQYQUZQMLDWIG6eJ0gOydks766zlLvPM7fRtV9/t3LvVtTYs85Qlq3fqPlXGtcYEMGyoNt1GrZvlr/LMg+MaOuNlZIfB9LxnVwGnlAbACeF7R3yzZF/CC47gBpsBp4WEy2vVBJuK8CxZGZmAjeReUdGiqSjuL8eTy82CbhRpd+Av3rZM9cfgWDR9dY929d0pub1EqI5G1ken7ll03Aw4WUfNIsDdLH/R5xzc/ehMpoP69d6WPBf+nnvuKZxh0VFXXUK0MqcsItdTUYBSYjDekWvZaY6z570z/s1Lo/gfEZ3XpGkIAOFHuPjOpewdiRUAAppK4KoGn/LkqPJ7Fq9kbNynnTUvZ7TKPVpdU0XfdcvQe7xuBp4EnJ2Z/VbAiXITt0Cpob7ySl47CpwEFYxByvY86YQVwj2H8FI5pM60Ak4PJbjUqsk7K+oKe5EFZAAJPrLowQ5cZZvAddddF5XcM7hkTM+iUsQBCHI25baVSX7uzHL5/xQqddAiu0U+R1G3HFX1XbccPcdLwNlJ7Xbn2K2A0+9kpqAPi5wr0FHglCDsqC4pDHhA6TesNMACZEScs8+3tgJObr2xWjXg17uksVF/kTsVQb6iiEMqG8FrxHECBhsEt8qzqeF1SHLRrzR25zKrR6p29V2PFP87Siu+q1P3LDpuFjEXNG12zF03y8/QkTcpvaeoUVJUN+30a8UL88TI7c8igdWOAicrE+cHyfGJctoETCQDs/SAVlbFMzhc9Sy/EKhZlFXqa3sDZ3asF8qhZ5MknCXNtzPZQ/K1dei7U89fJHJd5t42CPmNk046aam690b3kN8soRyV1Cyq3o78Un4UXtjgbfpopwUWWKCWvEkGDrlVARU93GRw6dl7L/m966PqmXueWYx2IYtLhr5Eb6CKe2OFDg7gFOGVHkSGqrWwjSzOMhM/kPrWoe9O6cvnP6T8qPNvFrkuc2/AqcLM1zXbPYDXe4FHbxW4akd+deRyftWKk5uVBajrKtnt5Cdp2tGz9Ed5zwBdBVHP1lVR9d4LKMt5dPq3BHCR6Syi3WngdCiBoI0E1yLR90ypXgb8YBbdJCcXHMdZ10Ir84IOKX3r0nenntfJ5goVFEMU+SZRp+RoNK6YAHBrZbFVlZ+HxOpCASj/1XiEDiIuc/p9M31kVTiCo1UqsjqpZ+CIMsRjwp8hBjgJ6tOycjelBfT8REAngVO0W+qJ6H2ZUi5lbU5/ca1SMA34s1rRDOiG3hPQyYkfUsauU9+deuYsHQ4Hh+v2naOqjfUnYCgQyIMqeq5no/uhN1jBPk3R6kCaduSX9E9Oh8Nw06XYMQbqWMs8SXENKXt06l3nvquka3f8dvScHZ7CUOtJq9G3WnsUIk+4qFFVO8fJlJZqJHpbhGTNFk/V6/IWOw5HPpyvMLIaixzmAByzrzj6aqIdqmcQibKl+Ni9cFrtLoi8ZxiSft8JfXfq+bluUsx4PlUPRfG8Mi2sE4etSG/r7QaWkR/ISMxnAQKbVq2q/LJDFH8AC7nI1nej9Loycmd9ca8sO+cn2JRggOpABkg770m7esbnkgmg96RmbPJA1eHRZeSrHTirKLtT12QRXQeKUFZe/qKEfd/KQSVYwKKC3KUyh9x26lmGhHGHNH17yVlHgLNq3q1NlnWI1+OhVD1pKZtfnhfqgPvc87zKRvNfRX6bvhp4FqtPXqCcJH8D4Z7WVpUzNcmoEgsdxivDmzYznpSM4sCbNacUKSnODJZ29Syibm7klddxBF6/Bk713U53sViqNEDLSnVYR2r5GhjS9M2KYdUtvPDCsQy3asM3AmABnXY/Wobf5Eo7hCbPcq0ivzMSABs6wNkR3g335AY796CIR9ZMTxm/KfAqZ5ilKRWxjCXXag6q6tkzomNscGU+kdJKln4NnFVfhHTdwNGAtBnVYAoyep8fW1QLPtgG6HBn/i7oVKUBQgFMll9RIC8rP+DkSqtwy4K3ovh4TvduBzgBG2uZZcdSBMSOcvOxNF/DLEPdNdJfVT3L2XZ/8+Oc1DpaAs46tJjGGGI1kH0byNkFLM8qzcHNrERHAzqiTkpSlQbAfC0ACBf1csrKz/pyyIVMEQEhYOdM2I022ijyfO00Y4rY+yIm6xNo4n6d2JV3+leR+1bVM4vaiWgKbqpy2b3lS8BZZMZSn9IakLMrulzUgnEWgL59UWkC9HCU+L6ylWSZYliLnreq/Dg8rr5E9DIfd3P/KvJnZcwyAeqq8On98UE8qp+ipdBFFllZPdtY0DECbg4Xr6v1a+DMarSlSNj5yrwUAh0Wl0kv+vLXNSlD+jgyDlgd3M08y8m3uaXycJfxhI4gHNzNPDu7QOZFmc+q1Ckn8POlxyzVrczY3SB/GXkHV1/vv2IXm3iZMymKyNevgZMCRMrVpsuJK1rRwWWyiOV9OoRZCkMeABRRdurTWANZYYQPnvUFcJJKWgpuDoCWOWawjjlVLCBdp52vO/al/HXooBNjiNrLGEB/VCmvbiVTvwdOpz4DQVHEIm6UlxiJLFEeSY9IB5rdVl3SiYVWx5jSTGw8WfRWGohDqhu1QYMGxSKIbgBO8mUfurNWipyQU4e+sqMIuelVy4AzOfpC/jp00Ikxss8XO+O3btAkb78GziwNwcfhuNsOT/BlSlxUs5d51llnjRFHPwhtxPJzzz0XSe92o4KdWCDdNqavM9qsUB24pSI66xbgpEuyCGzUycu1miP30+pK2Rnc8nfb+svkkYcqnTAvd7uq/P0aOLMqDAS1qg55cXkEPs5N6kJP4MyqITo1CVUnrxuvy87elFxt45HLB0x7NxuZz5aoMOsm4OxGnSaZuk8D/Ro4uS5SEBD+uA6Jv8CT+9jsZfZdZ9FVuXgOPUiuerlF6wBnOlPKR+/ZeavNRrG5yc9D4s8888wxLSbvmnISpd5JA/VroF8DZ8Zv+p66aKU8O2VcefwVjs55hfoqx3PIR1+cnF7/dKcRkwaSBurQQL8GznYU1K2nWLfzTOnapIGkgXo0kICzHj2mUZIGkgYGkAYScA6gyU6PmjSQNFCPBhJw1qPHNErSQNLAANJAAs4BNNnpUZMGkgbq0UACznr0mEZJGkgaGEAaSMA5gCY7PWrSQNJAPRpIwFmPHtMoSQNJAwNIA/8HA/3/IiTMMDMAAAAASUVORK5CYII="> </div>
<p>
where <i>k</i> indexes the number of developed arrow directions and <i>i</i> indexes the occupant tiles on the board. To illustrate this even better, 
the order of magnitude of the result of the outer summation only is <i>10^20</i>.
</p>
<p>
Another possible approach in RL is deep reinforcement learning [5]. This approach does not fit our needs as well because of the structure of project that is based on single web page that performs all the calculations online without the aid of external cloud services etc.
First and foremost, web-based calculations receive small amounts of memory from the browser, that the optimizes the overall performance. Thus, the depth of the potential model is highly restricted, while typical deep RL models somewhere from hundreds of MB up to few GB of memory.
The second reason was less practical for us, is the much larger data sets needed in deep approaches. Since our game is quite rare all the data for the training needs to be generated by us.
</p>
<p>
The value of each state is estimated by averaging the returns observed after visits to that state. Theoretically, by sampling infinity games, the sampled value will converge to the expected value according to the central limit theorem [3].
Naturally in our case, an episodic sampling means sampling full games. The return for all states observed in an episode is 1 for a win and -1 for a loss.  
</p>
<p>
The RL player is provided the game's state and all possible action for this state. Similarly to Q-learning methods we incorporate epsilon-greedy policy for exploration and exploitation. With a probability of epsilon  the algorithm chooses an explorative action by returning an action that leads to a yet visited state, or in case there are none unexplored neighboring states, it chooses the least visited one.
With (1-epsilon) probability the policy chooses an action that is optimal (according to its current best estimate of the optimal policy), and so it exploits the environment. If there are multiple neighboring states sharing the highest value, the algorithm picks one randomly.
</p>
<p>
A schematic view of the RL agent related logic, divided into training and inference stages, is shown in the figure below.
</p>
<br><div style="text-align: center;"><img src="/blog/images/blog_20_training.png" width="100%" style="text-align: center; max-width: 800px;"></div><br><br>
<h5 style="font-size: 14pt;">Game’s state encoding</h5>
<p>
with the directions added to them during the game. Since a piece may be captures during the game, its necessitates a special way to encode it. In addition, each piece has a color (flag) which indicates which of the players own it. Therefore, one can describe the state using an 80-dimensional array; Each 11 elements describe the following attributes: the first two values hold the <i>x</i> and <i>y</i> location coordinates. The additional eight values binary indicate if a direction is added to the piece. A special value (for example -1) indicates if the piece has been captured. 
</p>
<p>
It is possible to represent the game's state even more compactly using 16-dimensional array where each two adjacent pairs of values indicate a piece coordinates on the board (using a number between 0 and 41) and the directions added (using a number between 0 and 127 representing the eight binary number which indicates if a direction has been added or not). 
</p>
<h5 style="font-size: 14pt;">Action's encoding</h5>
<p>
The possible actions in the game are divided into two groups: adding a direction to a piece or moving it. Either way, an action is done on a specific piece according to its identification number. The first group requires one value describing the index of the direction. For the second group,  we encode the new location as an index on the board. Therefore, it is possible to describe any action using three dimensional array where the first value is the piece's Id, the second value indicates the type of action and the third value indicates the location's index on the board or the direction index - according to the value of the second value. 
</p>
<h5 style="font-size: 14pt;">Training</h5>
<p>
The training process of the model can be divided into two main stages: an offline learning and an online learning. In the offline learning stage, human and AI players played against each other, logging the states observed during the game and the winner's Id. Later, the data gathered was used to evaluate the state's initial value by the number of times a player visited this state and won/lost, divided by the number of times the state was observed. This process results an initial policy for the RL agent. Afterwards, the RL agent is played against the other AI agents including itself. After each game the states' values were updated.
</p>
<p>
After the offline learning is completed, the the model for the RL agent has been deployed online for human players to play with. Here, each player is allocated an Id number (using a web cookie) which stores the state and winner's (player or AI) in all the games the player played with the RL agent. When starting a new game with the RL agent, the state's scores of the model is updated online according to the game's played with this specific user. 
</p>
<h4>Evaluation</h4>
<p>
In order to evaluate each AI agent from game's prospective, we conducted a tournament between human and AI players. The tournament held as follows. We asked five different human players in different levels of mastery (first time player, player that played a few dozen games, the two developers of this AI, and a master player according to the Israeli OCTI society) to play with each one of the AI agents for 10 times. The order of the games with the AI agents were decided randomly for each human player by shuffling a list AI agents names. From each game, we stored if the AI won or not and how many actions it took for the AI to win (when it does). In addition, we compared the performance of the AI agents. Each AI agent is played 1000 times against each of the other players. Similarly, we stored if the AI won or not.
In order to eliminate the inherent advantage of the opening player, for each player, either human or AI, the number of times they played as the opening player was equal to the number of times they played as the second to play player.
</p>
<h3>Results</h3>
<p>
To illustrate the advantage of the RL based AI player we performed a series of tests as described in the previous section. Firstly, we ran the different AI agents to play against each other for a total sum of 2000 games, when the identity of the opening player is switched so each type of agent half of the times is playing first and half of times is playing second.
The runs were automatically conducted using  <b>aivsai.js</b> module. All the runs were made after the training period, in which the RL player reached some convergence. The results are graphed in the figures below.
As a sanity check, we examined the the graph bars related to the AI agents playing against themselves (e.g. greedy vs. greedy) and verified that indeed the win percentage is exactly 50% . The greedy player bars clearly show the dominance of the MINMAX and RL agents over it, with nearly zero winning percentages for the greedy player when playing against them. Looking at the bars of the MINMAX and RL agents we see a substantial improvement of the RL agent over the MINMAX, with win percentage 76%.
</p>
<p>
The next test was conducted using five volunteers with different levels of expertise in the game, starting with players first time playing and ending with a master player. The players played 10 games against each of the AI agents in a random order.
 As expected, a clear monotonous trend is reflected from the graph for all AI agents, where the higher the skill of the player the better the performance against the AI. More importantly, is that the win averages of the RL agent are higher than the other AI agents for all most all the players that participated, with a single exception of player 3, for which RL performance equaled to the MINMAX agent. What even more impressive is that the RL agent was the only AI agent that manged to beat the master player at least once, as shown by the bar on the right.
</p>
<br><div style="text-align: center;"><img src="/blog/images/blog_20_eval.png" width="100%" style="text-align: center; max-width: 800px;"></div><br><br>
<h3>Discussion</h3>
<p>
In our project we developed an online digitized version of the OCTI board game. The main challenges we faced at first when tackling the RL player development were all connected to the later realization of the uniqueness and complexity that characterise the general family of board games - "Multi agent adversarial problem".
</p>
<p>
Although, multi-agent reinforcement learning (MARL) has been studied for some time already, only in recent years this domain re-emerged due to advances in single-agent RL techniques [11].
 One of the main reasons for this, is the issue of scalability. It should be noted that OCTI can be analyzed as a two-player zero-sum setting, however, this can be misleading, since in our case each player has several pieces he can choose from and different type of actions for each piece. We speculate that these settings of the game are the main reason for the higher complexity of the problem compared to more classical RL solved problem like a race car on a track.
</p>
<p>
Looking at the results we conclude that our RL agent achieved some convergence, as otherwise it would not have outperformed the other AI agents and especially the MINMAX agent, which despite its deterministic nature play plays quite solidly against human players. We speculate that would our MINMAX agent were more effective the rate of convergence of the RL would have been faster since the former was the main source of initial training data for the RL. However, it was not practical to deepen the MINMAX further than 3 steps look-ahead since for already for 4 steps it would have taken an average of 87.55 seconds for each step of the AI, as seen in the table above.
Those times are closed to infinity in web based projects that rely on human interaction.
</p>
<p>
Our suggested future work, based on the platform we developed and built, is to employ the strengths of genetic Neuroevolution techniques for the development of stronger AI agent [9].
</p> <br>

<h3>References</h3>
<ol  style="margin-left: 20px; line-height: 1.5 !important; margin-bottom: 1.7rem; font-size: 1.1rem !important; font-weight: 400; color: #212529;">
	<li>Guillaume Chaslot et al. “Monte-Carlo Tree Search: A New Framework for Game AI.” in: 2008.</li>
	<li>Vladimir Fedorovich Demyanovand, Vasili Nikolaevich Malozemov. Introduction to minimax. Courier Corporation, 1990</li>
	<li>Richard M. Dudley. Uniform central limit theorems.volume 142. Cambridge university press, 2014.</li>
	<li>Chi Jinand et al. “Is Q-learning provably efficient?” in: arXiv preprint, (2018)</li>
	<li>Yuxi Li. “Deep reinforcement learning: An overview”.in: arXiv preprint, (2017)</li>
	<li>S. SantosoandI. Supriana. “Minimax guided reinforcement learning for turn-based strategygames”. in: 2014 2nd International Conference on Information and Communication Technol-ogy (ICoICT). 2014, pages 217–220.</li>
	<li>David  Silverand et al.  “A  general  reinforcement  learning  algorithm  that  masters  chess, shogi, and Go through self-play”.in: Science 362,6419 (2018), pages 1140–1144.</li>
	<li>David Silverand et al. “Mastering the game of go without human knowledge”.in: nature 550,7676 (2017), pages 354–359</li>
	<li>Felipe Petroski Suchandothers. “Deep neuroevolution: Genetic algorithms are a competitivealternative for training deep neural networks for reinforcement learning”. in: arXiv preprint, (2017)</li>
	<li>Thomas Walsh, Sergiu Goschinand, Michael Littman. “Integrating sample-based planningand model-based reinforcement learning”. in: Proceedings of the AAAI Conference on Artificial Intelligence 24,1 (2010)</li>
	<li>Kaiqing  Zhang,  Zhuoran  Yangand, Tamer  Basar.  “Multi-agent  reinforcement  learning:  Aselective overview of theories and algorithms”. in: arXiv preprint (2019)</li>
</ol>
